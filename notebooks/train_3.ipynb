{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset import *\n",
    "from features import *\n",
    "from train import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df = pd.read_parquet(\"../data/de_train.parquet\")\n",
    "\n",
    "train_index, val_index = stratified_split(de_df[\"cell_type\"], 0.2, 45)\n",
    "de_df_dataset_train = DataFrameDataset(de_df.iloc[train_index], mode=\"df\")\n",
    "de_df_dataset_val = DataFrameDataset(de_df.iloc[val_index], mode=\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtypes = list(set(de_df[\"sm_name\"].to_list()))\n",
    "mol_transforms = {\n",
    "    \"morgan_fp\": TransformList([Sm2Smiles(\"../config/sm_smiles.csv\", mode=\"path\"), Smiles2Mol(), Mol2Morgan()]),\n",
    "    \"one_hot\": TransformList([Type2OneHot(mtypes)])\n",
    "}\n",
    "\n",
    "ctypes = list(set(de_df[\"cell_type\"].to_list()))\n",
    "cell_transforms = {\n",
    "    \"one_hot\": TransformList([Type2OneHot(ctypes)])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dataset_train = DEDataset(de_df_dataset_train, mol_transforms, cell_transforms)\n",
    "de_dataset_val = DEDataset(de_df_dataset_val, mol_transforms, cell_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MODEL ####\n",
    "mol_enc = nn.Sequential(\n",
    "    nn.Linear(2048, 200)\n",
    ")\n",
    "cell_enc = nn.Sequential(\n",
    "    nn.Linear(len(ctypes), 10)\n",
    ")\n",
    "regressor = nn.Sequential(\n",
    "    nn.Linear(210, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(100, 500),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(500, len(de_df.columns)-5)\n",
    ")\n",
    "\n",
    "model = CombinerModel(\n",
    "        mol_encoder=mol_enc,\n",
    "        cell_encoder=cell_enc,\n",
    "        regressor=regressor)\n",
    "torch.save(model.state_dict(), \"temp/ini_model.pkl\")\n",
    "\n",
    "#### LOADERS, DATA ####\n",
    "train_dataloader = DataLoader(de_dataset_train, 256)\n",
    "val_dataloader = DataLoader(de_dataset_val, 256)\n",
    "\n",
    "#### CONFIG ####\n",
    "lr = 0.01\n",
    "epochs = 45\n",
    "device = \"cuda:0\"\n",
    "\n",
    "loss_fn = loss_fn\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=7)\n",
    "\n",
    "#### TENSORBOARD ####\n",
    "writer = SummaryWriter(\"./runs/kaggle/trying_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ghanb\\Desktop\\Main\\Projects\\Kaggle_SCP\\notebooks\\train_3.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ghanb/Desktop/Main/Projects/Kaggle_SCP/notebooks/train_3.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_many_epochs(model, train_dataloader, val_dataloader, epochs, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghanb/Desktop/Main/Projects/Kaggle_SCP/notebooks/train_3.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                   loss_fn, optimizer, scheduler, writer\u001b[39m=\u001b[39;49mwriter, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[1;32mc:\\Users\\ghanb\\Desktop\\Main\\Projects\\Kaggle_SCP\\notebooks\\..\\train.py:88\u001b[0m, in \u001b[0;36mtrain_many_epochs\u001b[1;34m(model, train_loader, val_loader, epochs, loss_fn, optimizer, scheduler, metrics, writer, device)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_many_epochs\u001b[39m(model, train_loader, val_loader, epochs,\n\u001b[0;32m     81\u001b[0m                       loss_fn, optimizer, scheduler\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \n\u001b[0;32m     82\u001b[0m                       metrics\u001b[39m=\u001b[39m[], writer\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     84\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     85\u001b[0m \n\u001b[0;32m     86\u001b[0m         \u001b[39m# Train model for one epoch and calculate metrics for the \u001b[39;00m\n\u001b[0;32m     87\u001b[0m         \u001b[39m# resulting model...\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m         train_b_losses \u001b[39m=\u001b[39m train_one_epoch(model, train_loader, loss_fn, \n\u001b[0;32m     89\u001b[0m                                          optimizer, device)\n\u001b[0;32m     90\u001b[0m         train_b_metrics \u001b[39m=\u001b[39m infer_model(model, train_loader, loss_fn, \n\u001b[0;32m     91\u001b[0m                                       metrics, device)\n\u001b[0;32m     92\u001b[0m         val_b_losses, val_b_metrics \u001b[39m=\u001b[39m infer_model(model, val_loader, loss_fn, \n\u001b[0;32m     93\u001b[0m                                     metrics, device, calculate_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ghanb\\Desktop\\Main\\Projects\\Kaggle_SCP\\notebooks\\..\\train.py:31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m x_batch, y_batch \u001b[39m=\u001b[39m batch\n\u001b[0;32m     30\u001b[0m y_batch \u001b[39m=\u001b[39m y_batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 31\u001b[0m y_pred \u001b[39m=\u001b[39m model(x_batch, device) \u001b[39m# TODO: Send to device the x in model?\u001b[39;00m\n\u001b[0;32m     33\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[0;32m     34\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\ghanb\\anaconda3\\envs\\pytorch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ghanb\\Desktop\\Main\\Projects\\Kaggle_SCP\\notebooks\\..\\model.py:24\u001b[0m, in \u001b[0;36mCombinerModel.forward\u001b[1;34m(self, x_mol, x_cell)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_mol, x_cell):\n\u001b[1;32m---> 24\u001b[0m   x_mol_enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmol_encoder(x_mol)\n\u001b[0;32m     25\u001b[0m   x_cell_enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_encoder(x_cell)\n\u001b[0;32m     26\u001b[0m   y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregressor(torch\u001b[39m.\u001b[39mconcat((x_mol_enc, x_cell_enc), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\ghanb\\anaconda3\\envs\\pytorch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ghanb\\anaconda3\\envs\\pytorch2\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ghanb\\anaconda3\\envs\\pytorch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ghanb\\anaconda3\\envs\\pytorch2\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "train_many_epochs(model, train_dataloader, val_dataloader, epochs, \n",
    "                  loss_fn, optimizer, scheduler, writer=writer, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
