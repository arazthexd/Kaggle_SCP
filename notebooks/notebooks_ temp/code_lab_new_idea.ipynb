{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "data_dir = \"../data\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyarrow.parquet import ParquetFile\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gene2Gene(nn.Module):\n",
    "    def __init__(self, gene_num, cell_num, d_embed_gene, d_embed_cell):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gene_num = gene_num\n",
    "        self.cell_num = cell_num\n",
    "        self.d_gene = d_embed_gene\n",
    "        self.d_cell = d_embed_cell\n",
    "        self.cell_emb = nn.Embedding(cell_num, d_embed_cell)\n",
    "        self.gene_emb = nn.Embedding(gene_num, d_embed_gene)\n",
    "        \n",
    "        self.layer_1 = nn.Linear(gene_num*2+cell_num+3, 20)\n",
    "        self.layer_2 = nn.Linear(20, 1)\n",
    "    \n",
    "    def __init__(self, gene1_idx, gene2_idx, cell_idx, x):\n",
    "        \n",
    "        gene1_embed = self.gene_emb(gene1_idx)\n",
    "        gene2_embed = self.gene_emb(gene2_idx)\n",
    "        cell_embed = self.cell_emb(cell_idx)\n",
    "        x = torch.concat([gene1_embed, gene2_embed, cell_embed, x], dim=1)\n",
    "\n",
    "        x = self.layer_2(nn.ReLU(self.layer_1(x)))\n",
    "        return x\n",
    "\n",
    "class MultiomeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, multiome_pf, multiome_var, multiome_obs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.raw_pf = multiome_pf\n",
    "        self.var_df = multiome_var\n",
    "        self.gene_names = self.var_df[self.var_df[\"feature_type\"]==\"Gene Expression\"][\"location\"].unique()\n",
    "        self.obs_df = multiome_obs\n",
    "    \n",
    "    def load(self, cell_names=\"all\", batch_num=10):\n",
    "\n",
    "        if cell_names == \"all\":\n",
    "            cell_names = self.obs_df[\"cell_type\"].unique()\n",
    "\n",
    "        multiome_obs_meta = self.obs_df\n",
    "        wanted_obs_ids = multiome_obs_meta[\n",
    "            multiome_obs_meta[\"cell_type\"].isin(cell_names)\n",
    "        ][\"obs_id\"].to_list()\n",
    "\n",
    "        self.multiome_df = pd.DataFrame()\n",
    "\n",
    "        multiome_file = self.raw_pf\n",
    "        batch_group_size = multiome_file.metadata.num_rows // batch_num\n",
    "        for batch in tqdm(multiome_file.iter_batches(batch_size=batch_group_size), total=batch_num+1):\n",
    "            multiome_batch = batch.to_pandas()\n",
    "            multiome_batch = multiome_batch[multiome_batch[\"obs_id\"].isin(wanted_obs_ids)]\n",
    "            multiome_batch = multiome_batch[multiome_batch[\"location\"].isin(self.gene_names)]\n",
    "            self.multiome_df = pd.concat([self.multiome_df, multiome_batch], axis=0, ignore_index=True)\n",
    "        \n",
    "        self.multiome_df.reset_index(inplace=True)\n",
    "    \n",
    "    def combine(self):\n",
    "\n",
    "        self.combined_idx = np.arange(len(self.gene_names)**2)\n",
    "        np.random.shuffle(self.combined_idx)\n",
    "\n",
    "    def choose(self):\n",
    "\n",
    "        chosen = np.random.choice(self.gene_names, 2)\n",
    "        return chosen[0], chosen[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.combined_idx.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        out_idx = self.combined_idx[idx]\n",
    "        if isinstance(out_idx, np.array):\n",
    "            for i in range(out_idx.shape[0]):\n",
    "                idx = out_idx[i]\n",
    "                idx1 = idx // len(self.gene_names)\n",
    "                idx2 = idx % len(self.gene_names)\n",
    "\n",
    "        elif isinstance(idx, int):\n",
    "            idx1 = out_idx // len(self.gene_names)\n",
    "            idx2 = out_idx % len(self.gene_names)\n",
    "            gene1 = self.gene_names[idx1]\n",
    "            gene2 = self.gene_names[idx2]\n",
    "            obs1 = self.multiome_df[self.multiome_df[\"location\"] == gene1].set_index(\"obs_id\")\n",
    "            obs2 = self.multiome_df[self.multiome_df[\"location\"] == gene2].set_index(\"obs_id\")\n",
    "            obs_common = list(set(obs1[\"obs_id\"]).intersection(set(obs2[\"obs_id\"])))\n",
    "            for j, obs in enumerate(obs_common):\n",
    "                exp1 = obs1.at[obs, gene1]\n",
    "                exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [02:03<00:00, 11.26s/it]\n"
     ]
    }
   ],
   "source": [
    "multiome_obs_meta = pd.read_csv(\"../../data/multiome_obs_meta.csv\")\n",
    "multiome_var_meta = pd.read_csv(\"../../data/multiome_var_meta.csv\")\n",
    "multiome_file = ParquetFile(\"../../data/multiome_train.parquet\")\n",
    "\n",
    "data = MultiomeDataset(multiome_file, multiome_var_meta, multiome_obs_meta)\n",
    "data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [0, 1, 2, 3]\n",
    "set(l).intersection(set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset import *\n",
    "from features import *\n",
    "from train import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "\n",
    "#### LOADERS, DATA ####\n",
    "de_dataset_train.configure(cell_out_feature=\"one_hot\", sm_out_feature=\"morgan2_fp\")\n",
    "de_dataset_val.configure(cell_out_feature=\"one_hot\", sm_out_feature=\"morgan2_fp\")\n",
    "train_dataloader = DataLoader(de_dataset_train, 32)\n",
    "val_dataloader = DataLoader(de_dataset_val, 32)\n",
    "\n",
    "#### MODEL ####\n",
    "model = BaselineModel(cell_in=len(ctypes), mol_in=2048, out_size=len(de_df.columns)-5)\n",
    "\n",
    "#### TRAINING ####\n",
    "lr = 0.02\n",
    "epochs = 500\n",
    "device = \"cuda:0\"\n",
    "\n",
    "loss_fn = loss_mrrmse\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=2e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=7)\n",
    "\n",
    "#### TENSORBOARD ####\n",
    "writer = SummaryWriter(\"./runs/trying_out2/2\")\n",
    "\n",
    "#### RUN ####\n",
    "train_many_epochs(model, train_dataloader, val_dataloader, epochs, \n",
    "                  loss_fn, optimizer, scheduler, writer=writer, device=device)\n",
    "\n",
    "os.chdir(\"./notebooks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheminfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
